{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HiPerCluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents\n",
    "\n",
    "- [**Step 1 Synthetic Fe-Cr APT Data Generator**](#step1)  \n",
    "  \n",
    "- [**Step 2 Data Preprocessing**](#step2)  \n",
    "  \n",
    "- [**Step 3 Model Training**](#step3)  \n",
    "  \n",
    "- [**Step 4 Inference**](#step4)  \n",
    "  \n",
    "- [**Step 5 Clustering**](#step5)  \n",
    "  \n",
    "- [**Step 6 Postprocessing**](#step6)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Synthetic Fe-Cr APT Data Generator <a class=\"anchor\" id=\"step1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating cluster sizes...\n",
      "Generating cluster centers...\n",
      "Centers generated: 100 (min distance = 8.55 nm)\n",
      "Generating Cr clusters...\n",
      "Cr points in clusters: 588000\n",
      "Remaining Cr for background: 252000\n",
      "Generating background points...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_845325/3694419827.py:143: UserWarning: Not enough background space. Reducing background atoms.\n",
      "  warnings.warn(\"Not enough background space. Reducing background atoms.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background: Cr = 250992, Fe = 6135358\n",
      "Data saved to ./SyntheticData/synthetic_clusters.mat\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import numpy as np\n",
    "from typing import Tuple, List\n",
    "from scipy.io import savemat\n",
    "from scipy.spatial.distance import pdist\n",
    "\n",
    "try:\n",
    "    from tqdm import trange\n",
    "except ImportError:\n",
    "    def trange(*args, **kwargs):\n",
    "        return range(*args, **kwargs)\n",
    "\n",
    "\n",
    "# ===================== USER INPUT SECTION =====================\n",
    "Dim = np.array([80.0, 80.0, 80.0]) # Volume dimensions [X, Y, Z] in nm\n",
    "TotalNumPoints = int(7000000) # Total number of atoms (Fe + Cr)\n",
    "CrPercentage = 0.12 # Chromium atomic fraction\n",
    "    \n",
    "Density = 9.5e23 # Atom density in atoms/m^3 (unused)\n",
    "DensityError = 0.2e23 # Density uncertainty (unused)\n",
    "    \n",
    "NumClusters = 100\n",
    "# clusterInfo: [NumClusters, rx_mean, rx_sigma, ry_mean, ry_sigma, rz_mean, rz_sigma]\n",
    "clusterInfo = np.array([NumClusters, 1.5, 0.2, 1.5, 0.2, 1.5, 0.1], dtype=float)\n",
    "    \n",
    "MinClusterSeparation = 5.0 # Minimum separation between cluster centers (nm)\n",
    "OutputFolder = './SyntheticData'\n",
    "os.makedirs(OutputFolder, exist_ok=True)\n",
    "    \n",
    "# Derived values\n",
    "TotalNumCr = int(round(CrPercentage * TotalNumPoints))\n",
    "TotalNumFe = TotalNumPoints - TotalNumCr\n",
    "    \n",
    "# Extra knobs\n",
    "RANDOM_SEED = 42\n",
    "F_CLUSTERED = 0.70\n",
    "MAX_CENTER_TRIES = 1000000\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# ===================== FUNCTIONS =====================\n",
    "    \n",
    "def generate_cluster_sizes(clusterInfo: np.ndarray) -> Tuple[np.ndarray, int, float]:\n",
    "    ci = np.asarray(clusterInfo, dtype=float)\n",
    "    if ci.ndim == 1:\n",
    "        ci = ci[None, :]\n",
    "    if ci.shape[1] != 7:\n",
    "        raise ValueError(\"clusterInfo must have 7 values: [count, baseX, deltaX, baseY, deltaY, baseZ, deltaZ]\")\n",
    "            \n",
    "    all_dims = []\n",
    "    for row in ci:\n",
    "        count = int(round(row[0]))\n",
    "        if count <= 0:\n",
    "                continue\n",
    "        baseX, deltaX, baseY, deltaY, baseZ, deltaZ = row[1:]\n",
    "        sizeX = baseX + (np.random.rand(count, 1) * 2 * deltaX) - deltaX\n",
    "        sizeY = baseY + (np.random.rand(count, 1) * 2 * deltaY) - deltaY\n",
    "        sizeZ = baseZ + (np.random.rand(count, 1) * 2 * deltaZ) - deltaZ\n",
    "        all_dims.append(np.hstack([sizeX, sizeY, sizeZ]))\n",
    "                    \n",
    "    clusterDims = np.vstack(all_dims) if all_dims else np.empty((0, 3), dtype=float)\n",
    "    numClusters = clusterDims.shape[0]\n",
    "    maxRadius = float(np.max(clusterDims)) if numClusters > 0 else 0.0\n",
    "    return clusterDims, numClusters, maxRadius\n",
    "\n",
    "\n",
    "def generateClusterCenters(xMin, xMax, yMin, yMax, zMin, zMax, numClusters, sepFactor, maxRadius):\n",
    "    xMin *= 0.8; xMax *= 0.8\n",
    "    yMin *= 0.8; yMax *= 0.8\n",
    "    zMin *= 0.8; zMax *= 0.8\n",
    "                            \n",
    "    candidateCount = int(1e6)\n",
    "    xLower, xUpper = xMin + maxRadius, xMax - maxRadius\n",
    "    yLower, yUpper = yMin + maxRadius, yMax - maxRadius\n",
    "    zLower, zUpper = zMin + maxRadius, zMax - maxRadius\n",
    "                            \n",
    "    if not (xLower < xUpper and yLower < yUpper and zLower < zUpper):\n",
    "        raise ValueError(\"Invalid bounds after applying maxRadius buffer.\")\n",
    "                                \n",
    "    randX = (xUpper - xLower) * np.random.rand(candidateCount) + xLower\n",
    "    randY = (yUpper - yLower) * np.random.rand(candidateCount) + yLower\n",
    "    randZ = (zUpper - zLower) * np.random.rand(candidateCount) + zLower\n",
    "                                \n",
    "    selectedX, selectedY, selectedZ = [randX[0]], [randY[0]], [randZ[0]]\n",
    "    minAllowedDist = sepFactor * maxRadius\n",
    "                                \n",
    "    for i in range(1, candidateCount):\n",
    "        dx = np.array(selectedX) - randX[i]\n",
    "        dy = np.array(selectedY) - randY[i]\n",
    "        dz = np.array(selectedZ) - randZ[i]\n",
    "        distances = np.sqrt(dx*dx + dy*dy + dz*dz)\n",
    "        if np.min(distances) >= minAllowedDist:\n",
    "            selectedX.append(randX[i])\n",
    "            selectedY.append(randY[i])\n",
    "            selectedZ.append(randZ[i])\n",
    "            if len(selectedX) >= numClusters:\n",
    "                break\n",
    "                                            \n",
    "    if len(selectedX) < numClusters:\n",
    "        raise RuntimeError(\"Not enough space for requested clusters.\")\n",
    "                                                \n",
    "    idx = np.random.permutation(len(selectedX))[:numClusters]\n",
    "    centers = np.column_stack([np.array(selectedX)[idx], np.array(selectedY)[idx], np.array(selectedZ)[idx]])\n",
    "                                                \n",
    "    minDistance = np.min(pdist(centers)) if len(centers) >= 2 else np.inf\n",
    "    return centers, minDistance\n",
    "\n",
    "\n",
    "def generate_cr_clusters(Centers, Radii, CrPercentage, TotalPoints):\n",
    "    total_cr = int(round(CrPercentage * TotalPoints))\n",
    "    n_clusters = Centers.shape[0]\n",
    "    max_per_cluster = total_cr // n_clusters\n",
    "    if max_per_cluster <= 0:\n",
    "        return np.empty((0, 4)), 0\n",
    "                                                        \n",
    "    pts_list = []\n",
    "    cr_used = 0\n",
    "    for i in range(n_clusters):\n",
    "        rx, ry, rz = Radii[i]\n",
    "        num_points = max_per_cluster\n",
    "        xyz = np.hstack([np.random.randn(num_points, 1) * rx + Centers[i, 0],\n",
    "                         np.random.randn(num_points, 1) * ry + Centers[i, 1],\n",
    "                         np.random.randn(num_points, 1) * rz + Centers[i, 2]])\n",
    "        labels = np.full((num_points, 1), i + 1, dtype=float)\n",
    "        pts_list.append(np.hstack([xyz, labels]))\n",
    "        cr_used += num_points\n",
    "                                                            \n",
    "    return np.vstack(pts_list), cr_used\n",
    "                                                            \n",
    "\n",
    "def generateBackground(Dim, Centers, dmax, NumCr, NumFe):\n",
    "    total = NumCr + NumFe\n",
    "    pts = np.random.rand(total, 3) * Dim\n",
    "    keep = np.ones(total, dtype=bool)\n",
    "    for c in Centers:\n",
    "        d = np.sqrt(np.sum((pts - c)**2, axis=1))\n",
    "        keep &= (d > dmax)\n",
    "    pts = pts[keep, :]\n",
    "                                                                \n",
    "    if pts.shape[0] < total:\n",
    "        warnings.warn(\"Not enough background space. Reducing background atoms.\")\n",
    "        total = pts.shape[0]\n",
    "        NumCr = int(round((NumCr / (NumCr + NumFe)) * total))\n",
    "        NumFe = total - NumCr\n",
    "                                                                    \n",
    "    return pts[:NumCr, :], pts[NumCr:NumCr + NumFe, :]\n",
    "\n",
    "\n",
    "# ===================== MAIN PIPELINE =====================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Generating cluster sizes...\")\n",
    "    Radii, numClusters, maxRadius = generate_cluster_sizes(clusterInfo)\n",
    "\n",
    "    print(\"Generating cluster centers...\")\n",
    "    Centers, minDist = generateClusterCenters(0, Dim[0], 0, Dim[1], 0, Dim[2],\n",
    "    numClusters, MinClusterSeparation, maxRadius)\n",
    "    print(f\"Centers generated: {Centers.shape[0]} (min distance = {minDist:.2f} nm)\")\n",
    "\n",
    "    print(\"Generating Cr clusters...\")\n",
    "    CrPoints_clusters, cr_used = generate_cr_clusters(Centers, Radii, F_CLUSTERED * CrPercentage, TotalNumPoints)\n",
    "    print(f\"Cr points in clusters: {CrPoints_clusters.shape[0]}\")\n",
    "\n",
    "    remainingCr = TotalNumCr - cr_used\n",
    "    print(f\"Remaining Cr for background: {remainingCr}\")\n",
    "\n",
    "    print(\"Generating background points...\")\n",
    "    Cr_bg, Fe_bg = generateBackground(Dim, Centers, maxRadius, remainingCr, TotalNumFe)\n",
    "    print(f\"Background: Cr = {Cr_bg.shape[0]}, Fe = {Fe_bg.shape[0]}\")\n",
    "\n",
    "    # Combine data\n",
    "    all_points = {\n",
    "        'Cr_clusters': CrPoints_clusters,\n",
    "        'Cr_background': Cr_bg,\n",
    "        'Fe_background': Fe_bg,\n",
    "        'Centers': Centers,\n",
    "        'Radii': Radii,\n",
    "        'Dim': Dim\n",
    "    }\n",
    "\n",
    "    # Save as .mat file\n",
    "    output_file = os.path.join(OutputFolder, \"synthetic_clusters.mat\")\n",
    "    savemat(output_file, all_points)\n",
    "    print(f\"Data saved to {output_file}\")\n",
    "    \n",
    "    # SAve Cr_clusters seperately as a .txt file\n",
    "    cr_clusters_file=os.path.join(OutputFolder,\"Cr_clusters.txt\")\n",
    "    np.savetxt(cr_clusters_file,CrPoints_clusters,fmt='%.6f')\n",
    "\n",
    "    # Optional: Visualization (sample only for speed)\n",
    "#     fig = plt.figure(figsize=(8, 6))\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "#     ax.scatter(CrPoints_clusters[:2000, 0], CrPoints_clusters[:2000, 1], CrPoints_clusters[:2000, 2], c='r', s=1, label='Cr Clusters')\n",
    "#     ax.scatter(Cr_bg[:2000, 0], Cr_bg[:2000, 1], Cr_bg[:2000, 2], c='g', s=1, label='Cr Background')\n",
    "#     ax.scatter(Fe_bg[:2000, 0], Fe_bg[:2000, 1], Fe_bg[:2000, 2], c='b', s=1, label='Fe Background')\n",
    "#     ax.set_xlim(0, Dim[0]); ax.set_ylim(0, Dim[1]); ax.set_zlim(0, Dim[2])\n",
    "#     ax.legend()\n",
    "#     plt.title(\"Synthetic Atom Distribution (Sample)\")\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Data Preprocessing <a class=\"anchor\" id=\"step2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.cm' has no attribute 'get_cmap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 136\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Save images (three views)\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mat \u001b[38;5;129;01min\u001b[39;00m (Count_xy, Count_xz, Count_yz):\n\u001b[0;32m--> 136\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mcount_rgb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresize_image_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     out_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00miter_counter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m     img\u001b[38;5;241m.\u001b[39msave(out_path, \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m, quality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m95\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 44\u001b[0m, in \u001b[0;36mcount_rgb\u001b[0;34m(img, out_size)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     43\u001b[0m     norm \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormalize(vmin\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, vmax\u001b[38;5;241m=\u001b[39mvmax, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 44\u001b[0m     cmap \u001b[38;5;241m=\u001b[39m \u001b[43mcm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_cmap\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviridis\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# pick a fixed, nice colormap\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     rgba \u001b[38;5;241m=\u001b[39m cmap(norm(img))         \u001b[38;5;66;03m# floats in [0,1], shape (H,W,4)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m rgb \u001b[38;5;241m=\u001b[39m (rgba[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m255.0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.cm' has no attribute 'get_cmap'"
     ]
    }
   ],
   "source": [
    "# Synthetic Data Subvolume Extraction and Image Generation (Python)\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from matplotlib import cm\n",
    "from matplotlib import colors\n",
    "from scipy.io import loadmat\n",
    "\n",
    "\n",
    "# ============================= USER INPUTS =============================\n",
    "base_folder = \"./SyntheticData\"   # <-- change to your base folder\n",
    "output_folder = \"Train\"                           # <-- folder to save generated images\n",
    "num_synthetic_files = 1                           # <-- total number of synthetic datasets\n",
    "cube_size = 20.0                                  # <-- subvolume size in nm\n",
    "grid_size = 100                                   # <-- grid size for projection images\n",
    "min_points_threshold = 300                        # <-- minimum number of points in subvolume to process\n",
    "min_cluster_size = 100                            # <-- minimum cluster size to consider\n",
    "resize_image_size = (100, 100)                    # <-- final image size (pixels) (width, height)\n",
    "\n",
    "# ======================================================================\n",
    " \n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    " \n",
    "iter_counter = 1\n",
    "parameters = []          # to mirror MATLAB \"parameter\" cell array (stores cluster_num)\n",
    "groundtruth = []         # rows: [syn, i, j, k, cluster_num]\n",
    " \n",
    "def count_rgb(img: np.ndarray, out_size=(100, 100)) -> Image.Image:\n",
    "    # \"\"\"\n",
    "    # Emulates MATLAB's imagesc + current colormap -> RGB, then resizes.\n",
    "    # - Uses matplotlib's 'viridis' by default (stable, perceptually uniform).\n",
    "    # - Scales to [0, max(img)], handling the all-zero case.\n",
    "    # \"\"\"\n",
    "    img = np.asarray(img, dtype=float)\n",
    "    vmax = float(img.max()) if img.size > 0 else 0.0\n",
    "    if vmax <= 0.0:\n",
    "        # All zeros -> produce a black image\n",
    "        rgba = np.zeros((img.shape[0], img.shape[1], 4), dtype=np.float32)\n",
    "    else:\n",
    "        norm = colors.Normalize(vmin=0.0, vmax=vmax, clip=True)\n",
    "        cmap = cm.get_cmap(\"viridis\")  # pick a fixed, nice colormap\n",
    "        rgba = cmap(norm(img))         # floats in [0,1], shape (H,W,4)\n",
    " \n",
    "    rgb = (rgba[..., :3] * 255.0).astype(np.uint8)\n",
    "    pil_img = Image.fromarray(rgb, mode=\"RGB\")\n",
    "    # MATLAB imresize default is bicubic-like; Pillow.BILINEAR is a reasonable match\n",
    "    return pil_img.resize(out_size, resample=Image.BILINEAR)\n",
    "\n",
    "\n",
    "for syn in range(1, num_synthetic_files + 1):\n",
    "#     folder_name = f\"GeneratedDatasetResults_Synthetic{syn}\"\n",
    "#     file_name = f\"Synthetic{syn}_CuAtomsInClusterandSolidSolution.txt\"\n",
    "   file_name='Cr_clusters.txt'\n",
    "   path = os.path.join(base_folder, file_name)\n",
    "\n",
    "\n",
    "   # Read the dataset (expecting numeric columns; 5th col is label)\n",
    "   # If your files have headers or different delimiters, adjust loadtxt arguments.\n",
    "   Data = np.loadtxt(path)\n",
    " \n",
    "\n",
    "   # Work only with spatial columns for cube tiling\n",
    "   mins = Data[:, :3].min(axis=0)\n",
    "   maxs = Data[:, :3].max(axis=0)\n",
    "   spans = maxs - mins\n",
    "\n",
    "   # Number of cubes along x, y, z\n",
    "   num_cubes = np.floor(spans / cube_size).astype(int)  # shape (3,)\n",
    "\n",
    "   # Guard: if any dimension has 0 cubes, nothing to do\n",
    "   if np.any(num_cubes <= 0):\n",
    "       continue\n",
    "\n",
    "   # Divide dataset into subvolumes \n",
    "   for i in range(1, num_cubes[0] + 1):\n",
    "       x0 = mins[0] + cube_size * (i - 1)\n",
    "       x1 = x0 + cube_size\n",
    "       in_x = (Data[:, 0] >= x0) & (Data[:, 0] <= x1)\n",
    "\n",
    "       for j in range(1, num_cubes[1] + 1):\n",
    "           y0 = mins[1] + cube_size * (j - 1)\n",
    "           y1 = y0 + cube_size\n",
    "           in_y = (Data[:, 1] >= y0) & (Data[:, 1] <= y1)\n",
    "\n",
    "           for k in range(1, num_cubes[2] + 1):\n",
    "               z0 = mins[2] + cube_size * (k - 1)\n",
    "               z1 = z0 + cube_size\n",
    "               in_z = (Data[:, 2] >= z0) & (Data[:, 2] <= z1)\n",
    "\n",
    "               gt = Data[in_x & in_y & in_z]\n",
    "\n",
    "               # Remove invalid points (label = -1) â€” assumes labels are in column 5 (0-based idx 4)\n",
    "               if gt.size == 0:\n",
    "                   continue\n",
    "               gt = gt[gt[:, 2] != -1]\n",
    "\n",
    "               if gt.shape[0] <= min_points_threshold:\n",
    "                   continue\n",
    "\n",
    "               # Count clusters >= min_cluster_size\n",
    "               labels = gt[:, 3].astype(int)\n",
    "               # Equivalent to MATLAB: [uniqueVals, ~, idx] + accumarray\n",
    "               unique_vals, counts = np.unique(labels, return_counts=True)\n",
    "               t_filtered = counts[counts >= min_cluster_size]\n",
    "               cluster_num = int(t_filtered.size)\n",
    "\n",
    "               # Scale coordinates to [0, grid_size - 1]\n",
    "               sub_min = gt[:, :3].min(axis=0)\n",
    "               sub_max = gt[:, :3].max(axis=0)\n",
    "               ranges = sub_max - sub_min\n",
    "               # avoid division by zero for flat ranges\n",
    "               ranges[ranges == 0] = 1.0\n",
    "               scaled = (gt[:, :3] - sub_min) / ranges * (grid_size - 1)\n",
    "\n",
    "               # Integer indices (MATLAB used floor + 1; here we keep 0-based)\n",
    "               idxs = np.floor(scaled).astype(int)\n",
    "               # clamp just in case of numerical edge (e.g., exactly grid_size)\n",
    "               np.clip(idxs, 0, grid_size - 1, out=idxs)\n",
    "\n",
    "               # Initialize projections\n",
    "               Count_xy = np.zeros((grid_size, grid_size), dtype=np.int32)\n",
    "               Count_xz = np.zeros((grid_size, grid_size), dtype=np.int32)\n",
    "               Count_yz = np.zeros((grid_size, grid_size), dtype=np.int32)\n",
    "\n",
    "               # Populate projections (vectorized with np.add.at)\n",
    "               ix, iy, iz = idxs[:, 0], idxs[:, 1], idxs[:, 2]\n",
    "               np.add.at(Count_xy, (ix, iy), 1)\n",
    "               np.add.at(Count_xz, (ix, iz), 1)\n",
    "               np.add.at(Count_yz, (iy, iz), 1)\n",
    "\n",
    "               # Save images (three views)\n",
    "               for mat in (Count_xy, Count_xz, Count_yz):\n",
    "                   img = count_rgb(mat, out_size=resize_image_size)\n",
    "                   out_path = os.path.join(output_folder, f\"{iter_counter}.jpg\")\n",
    "                   img.save(out_path, format=\"JPEG\", quality=95)\n",
    "\n",
    "                   parameters.append([cluster_num])\n",
    "                   groundtruth.append([syn, i, j, k, cluster_num])\n",
    "                   iter_counter += 1\n",
    "               \n",
    "\n",
    "# (Optional) Save metadata like MATLAB variables would\n",
    "# np.savetxt(os.path.join(output_folder, \"parameters.csv\"), np.array(parameters, dtype=int), fmt=\"%d\", delimiter=\",\")\n",
    "# np.savetxt(os.path.join(output_folder, \"groundtruth.csv\"), np.array(groundtruth, dtype=int), fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "print(f\"Done. Wrote {iter_counter-1} images to: {output_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3 Model Training <a class=\"anchor\" id=\"step3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#  Model training on ConvNeXt-Tiny\n",
    "# ============================================\n",
    "\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Dropout\n",
    "import tensorflow as tf\n",
    "import h5py\n",
    "\n",
    "import tensorflow\n",
    "tensorflow.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .mat file\n",
    "file = h5py.File('Train.mat', 'r')\n",
    "\n",
    "# Extract datasets\n",
    "XTrain = file['X_normalized'][:]  # Shape: (200, 200,200, 1, n)\n",
    "YTrain = file['Y_img'][:]  # Shape: (n, 2)\n",
    "\n",
    "XTrain_permuted=np.transpose(XTrain,(0,3,4,2,1))\n",
    "YTrain_permuted=np.transpose(YTrain,(1,0))\n",
    "\n",
    "print(XTrain_permuted.shape)\n",
    "print(YTrain_permuted.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pylab as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "#import tensorflow_hub as hub\n",
    "\n",
    "import datetime\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load the TensorBoard extension\n",
    "data = XTrain_permuted\n",
    "data = tf.squeeze(data, axis=4)\n",
    "print(data.shape)\n",
    "\n",
    "# Create the data augmentation generator\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.01,\n",
    "    height_shift_range=0.01,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data and YTrain_permuted are numpy arrays\n",
    "data = np.array(data)\n",
    "YTrain_permuted = np.array(YTrain_permuted)\n",
    "\n",
    "# Define the split ratio\n",
    "split_ratio = 0.2\n",
    "split_index = int((1 - split_ratio) * len(data))\n",
    "\n",
    "# Shuffling the dataset before splitting\n",
    "indices = np.random.permutation(len(data))\n",
    "train_indices = indices[:split_index]\n",
    "val_indices = indices[split_index:]\n",
    "\n",
    "train_data = data[train_indices]\n",
    "train_labels = YTrain_permuted[train_indices]\n",
    "val_data = data[val_indices]\n",
    "val_labels = YTrain_permuted[val_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.applications import ConvNeXtTiny\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Flatten, Dense, Conv2D, Resizing,GlobalAveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "# Load pre-trained ConvNeXtTiny model without the top layer\n",
    "base_model = ConvNeXtTiny(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "base_model.trainable=True\n",
    "\n",
    "# Create full model for fine-tunin\n",
    "model = Sequential()\n",
    "model.add(Resizing(224, 224, input_shape=(100,100,3)))\n",
    "model.add(base_model)\n",
    "model.add(GlobalAveragePooling2D())\n",
    "\n",
    "\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation='relu', kernel_regularizer=l2(1e-6)))\n",
    "model.add(Dropout(0.15))\n",
    "model.add(Dense(1,kernel_regularizer=l2(1e-6)))\n",
    "\n",
    "\n",
    "# Use a small learning rate for fine-tuning\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    verbose=1,\n",
    "    min_lr=1e-6\n",
    ")\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_true - y_pred)))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=opt, loss='mse', metrics=[tf.keras.metrics.MeanAbsoluteError()])\n",
    "\n",
    "# Setup early stopping to prevent over-fitting.\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model using the data augmentation generator\n",
    "import time\n",
    "start_time=time.time()\n",
    "history = model.fit(\n",
    "    datagen.flow(train_data, train_labels, batch_size=32),\n",
    "    epochs=100,\n",
    "    validation_data=(val_data, val_labels),\n",
    "    callbacks=[early_stopping,lr_reducer]\n",
    ")\n",
    "end_time=time.time()\n",
    "print(f\"Training time: {end_time-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss and accuracy values\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Plot loss on the left y-axis\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss', color='tab:blue')\n",
    "ax1.plot(history.history['loss'], label='Training Loss', color='tab:blue')\n",
    "if 'val_loss' in history.history:\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', color='tab:cyan')\n",
    "ax1.tick_params(axis='y', labelcolor='tab:blue')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Title and layout\n",
    "plt.title('Model Loss and Accuracy')\n",
    "fig.tight_layout()\n",
    "\n",
    "# Save the plot as an image file\n",
    "plt.savefig('Convtiny.png')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_data = history.history\n",
    "import scipy.io\n",
    "scipy.io.savemat('Convtiny_history.mat', history_data)\n",
    "\n",
    "model.save('ConvTiny.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4 Inference <a class=\"anchor\" id=\"step4\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "\n",
    "# Path to the folder containing .jpg images\n",
    "image_folder = './'\n",
    "image_files = sorted([f for f in os.listdir(image_folder) if f.endswith('.jpg')])\n",
    "\n",
    "# Load all images into a numpy array\n",
    "X = np.array([\n",
    "    np.array(Image.open(os.path.join(image_folder, fname)).resize((100, 100)))\n",
    "    for fname in image_files\n",
    "])\n",
    "X = X / 255.0  # if your model expects normalized input\n",
    "\n",
    "# Load the model and specify the custom objects\n",
    "loaded_model = load_model('ConvTiny.keras') \n",
    "# OR\n",
    "loaded_model = load_model('ResNet50.keras')\n",
    "\n",
    "# Predict\n",
    "predictions = loaded_model.predict(X)\n",
    "print(predictions)\n",
    "\n",
    "import scipy.io\n",
    "scipy.io.savemat('predictions.mat', {'predictions': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5 Clustering <a class=\"anchor\" id=\"step5\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "#  K-Means + HDBSCAN Clustering Script (Python)\n",
    "#  Description:\n",
    "#    - Loads raw 3D coordinates and model predictions \n",
    "#    - Preliminary clustering with K-Means\n",
    "#    - HDBSCAN refinement\n",
    "# ============================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import numpy as np\n",
    "import subprocess\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any, Optional\n",
    "import h5py\n",
    "from scipy.io import loadmat\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 1) USER INPUTS\n",
    "# ================================\n",
    "rawDataFile      = \"subvolume.txt\"               # <-- Raw 3D coordinates .mat; expects a variable containing Nx3 coords\n",
    "predictionFile   = \"predictions.mat\"   # <-- .mat with model predictions (vector or scalar)\n",
    "threshold        = 2.0                      # <-- distance threshold for assigning clusters\n",
    "minPersistence   = 0.06                     # <-- persistence cutoff for HDBSCAN filtering\n",
    "\n",
    "\n",
    "# Choose HDBSCAN mode:\n",
    "HDBSCAN_MODE     = \"native\"                 # \"native\" (uses hdbscan lib) or \"external\" (call your script)\n",
    "\n",
    "\n",
    "# External-script settings (used only if HDBSCAN_MODE == \"external\")\n",
    "# pythonEnvPath    = \"/home/tangy/miniforge3/envs/APT/bin/python\"  # path to python exe\n",
    "# hdbscanScript    = \"hdbscanImanSecond_9.py\"                      # your external script\n",
    "tempDataFile     = \"TempHDBSCANfile.txt\"\n",
    "tempParamFile    = \"TempHDBSCANparameters.txt\"\n",
    "\n",
    "\n",
    "def _counts_from_positive_labels(labels: np.ndarray) -> np.ndarray:\n",
    "    #\"\"\"\n",
    "    #Equivalent to MATLAB accumarray(clusterLabels(clusterLabels>0), 1)\n",
    "    #\"\"\"\n",
    "    pos = labels[labels > 0]\n",
    "    if pos.size == 0:\n",
    "        return np.array([], dtype=int)\n",
    "    m = int(pos.max())\n",
    "    counts = np.bincount(pos, minlength=m + 1)  # index 0 unused\n",
    "    return counts[1:]  # drop 0\n",
    "\n",
    "\n",
    "# ================================\n",
    "# 2) LOAD DATA\n",
    "# ================================\n",
    "print(\"Loading data...\")\n",
    "#raw_mat = loadmat(rawDataFile)\n",
    "#pred_mat = loadmat(predictionFile)\n",
    "\n",
    "gt=np.loadtxt(rawDataFile)\n",
    "pred_mat = loadmat(predictionFile)\n",
    "predictions = pred_mat['predictions']\n",
    "pred_n = np.round(predictions[0][0])\n",
    "\n",
    "\n",
    "\n",
    "# KMeans over coordinates\n",
    "kmeans = KMeans(n_clusters=int(pred_n), n_init=\"auto\", random_state=0)\n",
    "kmeans.fit(gt[:, :3])\n",
    "C = kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "# Compute distances to centroids, assign closest, mark outliers by threshold\n",
    "D = cdist(gt[:, :3], C, metric=\"euclidean\")\n",
    "minDist = D.min(axis=1)\n",
    "idx = D.argmin(axis=1)\n",
    "\n",
    "\n",
    "clusterLabels = idx.astype(int) + 1  # make them 1..K like MATLAB\n",
    "clusterLabels[minDist > threshold] = -1\n",
    "\n",
    "\n",
    "counts = _counts_from_positive_labels(clusterLabels)\n",
    "label = clusterLabels.copy()\n",
    "print(f\"Preliminary clustering complete. Found {len(counts)} clusters.\")\n",
    "\n",
    "# ================================\n",
    "# 4) DETERMINE HDBSCAN PARAMETERS\n",
    "# ================================\n",
    "if counts.size == 0:\n",
    "    # no clusters survived the threshold; nothing to refine\n",
    "    print(\"No valid clusters from K-Means. Exiting early.\")\n",
    "    hdbscanCluster_CNN: List[Cluster] = []\n",
    "else:\n",
    "    m = int(np.min(counts))            # min_cluster_size\n",
    "    n = int(np.round(0.1 * m))         # min_samples\n",
    "    print(f\"HDBSCAN parameters: min_cluster_size = {m}, min_samples = {n}\")\n",
    "\n",
    "# ================================\n",
    "# 5) HDBSCAN \n",
    "# ================================\n",
    "import sys\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Save the first 3 columns of gt to a text file with space delimiter\n",
    "np.savetxt(tempDataFile, gt[:, :3], delimiter=' ', fmt='%.6f')\n",
    "\n",
    "# Write m and n to a parameter file\n",
    "with open(tempParamFile, 'w') as f:\n",
    "    f.write(f\"{m}\\n\")\n",
    "    f.write(f\"{n}\")\n",
    "\n",
    "    \n",
    "X=np.genfromtxt(tempDataFile)\n",
    "Y=np.genfromtxt(tempParamFile)\n",
    "MinClusterSize=int(Y[0])\n",
    "MinSamples=int(Y[1])\n",
    "\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=MinClusterSize,min_samples=MinSamples,\n",
    "                            cluster_selection_method='eom',approx_min_span_tree=False,core_dist_n_jobs=1)\n",
    "clusterer.fit(X)\n",
    "tree=clusterer.condensed_tree_.plot(select_clusters=True,selection_palette=sns.color_palette())\n",
    "np.savetxt('Labels.txt',clusterer.labels_, fmt='%d',comments='')\n",
    "np.savetxt('Persistence.txt',clusterer.cluster_persistence_,comments='')\n",
    "np.savetxt('Probabilities.txt',clusterer.probabilities_,comments='')  \n",
    "\n",
    "\n",
    "# ================================\n",
    "# 6) PROCESS HDBSCAN RESULTS (native)\n",
    "# ================================\n",
    "\n",
    "# Load data from text files\n",
    "hdbscan_labels = np.loadtxt('Labels.txt')\n",
    "hdbscan_persistence = np.loadtxt('Persistence.txt')\n",
    "hdbscan_probabilities = np.loadtxt('Probabilities.txt')\n",
    "PositionDataset = np.loadtxt(tempDataFile)\n",
    "SiZePositionDatasetCol=PositionDataset.shape[1] \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "NoiseLabel = -1\n",
    "UniqueLabels = np.unique(hdbscan_labels)\n",
    "\n",
    "hdbscanCluster = []\n",
    "\n",
    "# Determine if noise label is present\n",
    "has_noise = NoiseLabel in UniqueLabels\n",
    "\n",
    "# Adjust loop range depending on presence of noise\n",
    "loop_range = range(len(UniqueLabels) - 1) if has_noise else range(len(UniqueLabels))\n",
    "\n",
    "for i in loop_range:\n",
    "    label = i\n",
    "    # Find indices where hdbscan_labels == i\n",
    "    row_indices = np.where(hdbscan_labels == label)[0]\n",
    "\n",
    "    cluster_info = {\n",
    "        'labels': label,\n",
    "        'probabilities': hdbscan_probabilities[row_indices],\n",
    "        'persistence': hdbscan_persistence[i],\n",
    "        'atomPositions': PositionDataset[row_indices, :SiZePositionDatasetCol],\n",
    "        'clustersize': len(row_indices)\n",
    "    }\n",
    "\n",
    "    hdbscanCluster.append(cluster_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 Postprocessing <a class=\"anchor\" id=\"step6\"></a>\n",
    "The following python script demonstrates how to compute number density, cluster radius, and volume fraction using the clusters structure returned by processHDBSCANResults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ================================\n",
    "# Post-Processing of Cluster Properties\n",
    "# ================================\n",
    "\n",
    "# INPUTS:\n",
    "# hdbscanCluster - list of dictionaries from previous step\n",
    "# subvolume_size - volume of the analyzed region (in nm^3)\n",
    "# PositionDataset - all points in the analyzed region (Nx3 matrix)\n",
    "\n",
    "# ---- Parameters ----\n",
    "Q = 0.36 # Detection efficiency\n",
    "rho = 84.3 # Atomic density of alpha-Fe (atoms/nm^3)\n",
    "CrPercent = 0.09 # Cr percentage in the raw 3D data (e.g., 9%)\n",
    "subvolume_size=40^3 # subvolume size in nm^3\n",
    "\n",
    "# ---- Number Density (clusters per m^3) ----\n",
    "num_clusters = len(hdbscanCluster)\n",
    "number_density = num_clusters / subvolume_size *1e27 # clusters per m^3\n",
    "\n",
    "# ---- Cluster Radius for Each Cluster (nm) ----\n",
    "cluster_radii = np.zeros(num_clusters)\n",
    "for i, cluster in enumerate(hdbscanCluster):\n",
    " num_atoms = cluster['clustersize'] # Number of atoms in the cluster\n",
    " cluster_radii[i] = ((3 * num_atoms) / (4 * np.pi * rho * Q)) ** (1/3)\n",
    "\n",
    "# ---- Volume Fraction of Clusters ----\n",
    "total_cluster_atoms = sum(cluster['clustersize'] for cluster in hdbscanCluster)\n",
    "total_atoms = PositionDataset.shape[0]\n",
    "volume_fraction = total_cluster_atoms / (total_atoms / CrPercent)\n",
    "\n",
    "# ---- Display Results ----\n",
    "print(f\"Number Density: {number_density:.4e} clusters/m^3\")\n",
    "print(f\"Average Cluster Radius: {np.mean(cluster_radii):.3f} nm\")\n",
    "print(f\"Volume Fraction: {volume_fraction:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple python code to visualize the clustersusing different colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Example: replace this with your actual data\n",
    "# data = np.loadtxt('your_data.txt')  # or however you load your nx4 matrix\n",
    "# Assuming data is an (n x 4) numpy array\n",
    "# Columns: x, y, z, cluster_label\n",
    "# Example dummy data:\n",
    "data = np.loadtxt('example_plot.txt')\n",
    "\n",
    "def plot_clusters(data):\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    x, y, z = data[:, 0], data[:, 1], data[:, 2]\n",
    "    labels = data[:, 3].astype(int)\n",
    "\n",
    "    for cluster_id in np.unique(labels):\n",
    "        mask = labels == cluster_id\n",
    "        ax.scatter(x[mask], y[mask], z[mask], label=f'Cluster {cluster_id}', s=20)\n",
    "\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    # ax.legend()\n",
    "    plt.title('3D Cluster Visualization')\n",
    "    plt.show()\n",
    "\n",
    "# Call the function with your data\n",
    "plot_clusters(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Tensorflow 2.16.1",
   "language": "python",
   "name": "tensorflow2.16.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
